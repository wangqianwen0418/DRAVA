model_params:
  name: 'BetaVAEMLP'
  in_channels: 4096
  latent_dim: 10 # for simple shape, same as the beta vae paper
  hidden_dims: [1200, 1200]
  loss_type: 'B'
  gamma: 100.0
  max_capacity: 25
  Capacity_max_iter: 10000

exp_params:
  # dataset: celeba
  optimizer: "adagrad"
  dataset: sunspots
  data_path: "../../Data/"
  img_size: 64
  batch_size: 144 # Better to have a square number
  LR: 0.01
  weight_decay: 0.0
  scheduler_gamma: 0.95

trainer_params:
  gpus: 1
  max_nb_epochs: 100
  max_epochs: 100

logging_params:
  save_dir: "logs/"
  name: "BetaVAE_B_MLP"
  manual_seed: 1265
