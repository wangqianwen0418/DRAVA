model_params:
  name: 'BetaVAE_CONV2'
  in_channels: 1
  latent_dim: 6 # for dsprites
  # latent_dim: 28 # for celeba
  img_size: 64
  loss_type: 'B'
  gamma: 100.0
  max_capacity: 25
  Capacity_max_iter: 10000
  recons_multi: 1
  distribution: bernoulli

exp_params:
  # dataset: celeba
  dataset: HFFc6_ATAC_chr1-8
  data_path: "./data/"
  img_size: 64
  optimizer: "adam"
  batch_size: 256 # Better to have a square number
  LR: 0.0005
  weight_decay: 0.0
  # scheduler_gamma: 0.95 # if no scheduler gama, then reduce learning rate on plateau

trainer_params:
  gpus: 1
  max_nb_epochs: 200
  max_epochs: 100

logging_params:
  save_dir: "logs"
  name: 'BetaVAE_CONV'
  manual_seed: 1265
