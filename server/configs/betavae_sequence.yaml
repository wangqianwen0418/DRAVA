model_params:
  name: 'BetaVAE_CONV'
  in_channels: 1
  latent_dim: 5 # for dsprites
  # latent_dim: 28 # for celeba
  img_size: 64
  hidden_dims: [8, 16, 32, 64, 128]
  loss_type: 'B'
  gamma: 10.0
  max_capacity: 25
  Capacity_max_iter: 10000

exp_params:
  # dataset: celeba
  dataset: HFFc6_ATAC_chr7
  data_path: "./data/"
  img_size: 64
  optimizer: "adam"
  batch_size: 256 # Better to have a square number
  LR: 0.0005
  weight_decay: 0.0
  # scheduler_gamma: 0.95 # if no scheduler gama, then reduce learning rate on plateau

trainer_params:
  gpus: 1
  max_nb_epochs: 200
  max_epochs: 2000

logging_params:
  save_dir: "logs"
  name: 'BetaVAE_CONV'
  manual_seed: 1265
